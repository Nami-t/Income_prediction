
from sklearn.model_selection import GridSearchCV
param_grid= {
    'n_estimators':[50,100,250],
    'max_depth':[5,10,30,None],
    'min_samples_split':[2,4],
    'max_features':['sqrt','log2']
}
grid_search = GridSearchCV(estimator= RandomForestClassifier(),
param_grid= param_grid, verbose=10)
import pandas as pd
df  = pd.read_csv('income.csv')
df
df.education.value_counts()
import pandas as pd

# List of columns to one-hot encode
categorical_columns = ['occupation', 'workclass', 'marital-status', 'relationship', 'race', 'native-country']

# One-hot encode all the categorical columns in one go
df = pd.get_dummies(df, columns=categorical_columns, prefix=categorical_columns)

# Drop the 'education' column as per your original logic
df = df.drop('education', axis=1)
df
df['gender']=df['gender'].apply(lambda x: 1 if x=='Male' else 0)
df['income']=df['income'].apply(lambda x: 1 if x=='>50K' else 0)
df
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(18,12))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm')
df.corr()
correlations= df.corr()['income'].abs()
sorted_correlations=correlations.sort_values()
num_cols_to_drop = int(0.8 * len(df.columns))
cols_to_drop = sorted_correlations.iloc[:num_cols_to_drop].index
df_dropped= df.drop(cols_to_drop, axis=1)
df_dropped
plt.figure(figsize=(15,10))
sns.heatmap(df_dropped.corr(), annot=False, cmap='coolwarm')
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

df= df.drop('fnlwgt', axis= 1)

train_df ,test_df = train_test_split(df, test_size=0.2)
train_df
test_df
train_X = train_df.drop('income', axis=1 )
train_Y = train_df['income']

test_X = test_df.drop('income', axis= 1)
test_Y = test_df['income']
forest = RandomForestClassifier()

forest.fit(train_X, train_Y)
forest.score(test_X, test_Y)
forest.feature_importances_
forest.feature_names_in_
importances = dict(zip(forest.feature_names_in_ ,forest.feature_importances_))
importances = {k: v for k , v in sorted(importances.items(), key= lambda x: x[1] , reverse=True)}
importances
from sklearn.model_selection import GridSearchCV
param_grid= {
    'n_estimators':[50,100,250],
    'max_depth':[5,10,30,None],
    'min_samples_split':[2,4],
    'max_features':['sqrt','log2']
}
grid_search = GridSearchCV(estimator= RandomForestClassifier(),
param_grid= param_grid, verbose=10)
grid_search.fit(train_X,train_Y)
grid_search.fit(train_X,train_Y)
grid_search.best_estimator_
forest = grid_search.best_estimator_
forest.score(test_X, test_Y)
importances = dict(zip(forest.feature_names_in_ ,forest.feature_importances_))
importances = {k: v for k , v in sorted(importances.items(), key= lambda x: x[1] , reverse=True)}
importances
from IPython.core.history import HistoryManager

# Access the history database
ipython_history = HistoryManager()
for session, line_num, code in ipython_history.get_range():
    print(f"Session: {session}, Line: {line_num}, Code: {code}")
from IPython.core.history import HistoryManager

# Access the history database
ipython_history = HistoryManager()
for session, line_num, code in ipython_history.get_range():
    print(f"Session: {session}, Line: {line_num}, Code: {code}")
get_ipython().run_line_magic('history', '')
get_ipython().run_line_magic('history', '-n 1-32')
import pandas as pd
get_ipython().run_line_magic('history', '-n 1-32')
with open('history.txt', 'w') as f:
    for item in In:  # `In` contains the input history.
        f.write("%s\n" % item)
